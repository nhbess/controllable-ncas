{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af179e3c-3a9f-4c84-985a-e9656c6cf44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89494104-eb29-4588-8948-3fdb25bb4b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from controllable_nca.dataset import NCADataset\n",
    "from controllable_nca.utils import load_emoji, rgb\n",
    "\n",
    "def plot_img(img):\n",
    "    with torch.no_grad():\n",
    "        rgb_image = rgb(img, False).squeeze().detach().cpu().numpy()\n",
    "    rgb_image = rearrange(rgb_image, \"c w h -> w h c\")\n",
    "    _ = plt.imshow(rgb_image)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e4d518d-1a9f-42db-8e17-56669c8edef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from controllable_nca.dataset import NCADataset\n",
    "\n",
    "from controllable_nca.dataset import MultiClass2DDataset\n",
    "from controllable_nca.utils import load_emoji, rgb\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EmojiDataset(MultiClass2DDataset):\n",
    "    # EMOJI = 'ðŸ¦ŽðŸ˜€ðŸ’¥'\n",
    "    EMOJI = \"ðŸ¦Ž\"\n",
    "\n",
    "    def __init__(self, image_size=48, padding=16):\n",
    "        center = F.pad(\n",
    "            torch.stack(\n",
    "                [load_emoji(e, image_size) for e in EmojiDataset.EMOJI], dim=0\n",
    "            ),\n",
    "            (padding // 2, padding // 2, padding // 2, padding // 2, 0, 0)\n",
    "        )\n",
    "        left = F.pad(center, (0, padding, 0, 0, 0, 0, 0, 0))[:, :, :, padding:]\n",
    "        right = F.pad(center, (padding, 0, 0, 0, 0, 0, 0, 0))[:, :, :, :-padding]\n",
    "        emojis = torch.stack([center,left,right]).squeeze()\n",
    "        self.padding = padding\n",
    "        targets = torch.arange(emojis.size(0))\n",
    "        super(EmojiDataset, self).__init__(emojis, targets, use_one_hot=True)\n",
    "\n",
    "    def visualize(self, idx=0):\n",
    "        self.plot_img(self.x[idx : idx + 1])\n",
    "\n",
    "    def plot_img(self, img):\n",
    "        with torch.no_grad():\n",
    "            rgb_image = rgb(img, False).squeeze().detach().cpu().numpy()\n",
    "        rgb_image = rearrange(rgb_image, \"c w h -> w h c\")\n",
    "        _ = plt.imshow(rgb_image)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f869ac1a-669d-4bec-ba92-897dd6342414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_in_grid(img, x = None, y = None, grid_size = 64):\n",
    "    with torch.no_grad():\n",
    "        if x is None:\n",
    "            x = grid_size // 2\n",
    "        if y is None:\n",
    "            y = grid_size // 2\n",
    "\n",
    "        img_size = img.size(-1)\n",
    "        center = img_size // 2\n",
    "        grid = torch.zeros(1, img.size(1), grid_size, grid_size, device=img.device)\n",
    "\n",
    "        min_x = x - center\n",
    "        min_x_diff = 0 - min(0, min_x)\n",
    "        max_x = x + center\n",
    "        max_x_diff = grid_size - max(64, max_x)\n",
    "        min_x = min_x + max_x_diff + min_x_diff\n",
    "        max_x = max_x + max_x_diff + min_x_diff\n",
    "\n",
    "        min_y = y - center\n",
    "        min_y_diff = 0 - min(0, min_y)\n",
    "        max_y = y + center\n",
    "        max_y_diff = grid_size - max(grid_size, max_y)\n",
    "        min_y = min_y + max_y_diff + min_y_diff\n",
    "        max_y = max_y + max_y_diff + min_y_diff\n",
    "\n",
    "        grid[:, :, min_x:max_x, min_y:max_y] += img\n",
    "        return grid, (min_x + center, min_y + center)\n",
    "\n",
    "class EmojiPlacementDataset(NCADataset):\n",
    "    EMOJI = \"ðŸ¦Ž\"\n",
    "\n",
    "    def __init__(self, grid_size=64, image_size=32):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.image_size = image_size\n",
    "        self.x = load_emoji(EmojiPlacementDataset.EMOJI, image_size).unsqueeze(0)\n",
    "\n",
    "    def draw(self, x=None, y=None, substrate=None):\n",
    "        if substrate is None:\n",
    "            substrate = self.x\n",
    "        return draw_in_grid(substrate.clone(), x=x,y=y, grid_size=self.grid_size)\n",
    "\n",
    "    def target_size(self):\n",
    "        return (4, self.grid_size, self.grid_size)\n",
    "\n",
    "    def visualize(self, x=None, y=None):\n",
    "        grid, coords = self.draw(x,y)\n",
    "        plot_img(grid)\n",
    "        \n",
    "    def to(self, device: torch.device):\n",
    "        self.x = self.x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59bd1297-8164-4626-ae39-ed16d4a3e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EmojiPlacementDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6648cd84-9f1e-4835-b86f-2e737c3ce5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.visualize(48, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eacf1d4-c37f-45c4-a679-2490d1e7d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from controllable_nca.image.nca import ControllableImageNCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c2126d5-fa0a-4077-86e9-6418fc859246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import Any, Optional, Tuple  # noqa\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "from controllable_nca.dataset import NCADataset\n",
    "from controllable_nca.image.nca import ControllableImageNCA\n",
    "from controllable_nca.sample_pool import SamplePool\n",
    "from controllable_nca.trainer import NCATrainer\n",
    "from controllable_nca.utils import create_2d_circular_mask\n",
    "\n",
    "\n",
    "class ControllableNCAImageTrainer(NCATrainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nca: ControllableImageNCA,\n",
    "        target_dataset: NCADataset,\n",
    "        nca_steps=[48, 96],\n",
    "        lr: float = 2e-3,\n",
    "        pool_size: int = 512,\n",
    "        num_damaged: int = 0,\n",
    "        log_base_path: str = \"tensorboard_logs\",\n",
    "        damage_radius: int = 3,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ):\n",
    "        super(ControllableNCAImageTrainer, self).__init__(\n",
    "            pool_size, num_damaged, log_base_path, device\n",
    "        )\n",
    "        self.target_dataset = target_dataset\n",
    "        self.target_size = self.target_dataset.target_size()\n",
    "\n",
    "        self.nca = nca\n",
    "        self.min_steps = nca_steps[0]\n",
    "        self.max_steps = nca_steps[1]\n",
    "\n",
    "        self.num_target_channels = self.target_size[0]\n",
    "        self.image_size = self.target_size[-1]\n",
    "        self.rgb = self.target_size[0] == 3\n",
    "        self.damage_radius = damage_radius\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.nca.parameters(), lr=lr)\n",
    "        self.lr_sched = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            self.optimizer, [5000], 0.3\n",
    "        )\n",
    "\n",
    "    def to_alpha(self, x):\n",
    "        return torch.clamp(x[:, 3:4, :, :], 0.0, 1.0)  # 1.0\n",
    "\n",
    "    def to_rgb(self, x):\n",
    "        # assume rgb premultiplied by alpha\n",
    "        if self.rgb:\n",
    "            return torch.clamp(x[:, :3], 0.0, 1.0).detach().cpu().numpy()\n",
    "        rgb = x[:, :3, :, :]  # 0,0,0\n",
    "        a = self.to_alpha(x)  # 1.0\n",
    "        im = 1.0 - a + rgb  # (1-1+0) = 0, (1-0+0) = 1\n",
    "        im = torch.clamp(im, 0, 1)\n",
    "        return im.detach().cpu().numpy()\n",
    "\n",
    "    def loss(self, x, targets):\n",
    "        return F.mse_loss(\n",
    "            x[:, : self.num_target_channels, :, :], targets[:, : self.num_target_channels, :, :], reduction=\"none\"\n",
    "        ).mean(dim=(1, 2, 3))\n",
    "\n",
    "    def sample_batch(self, sampled_indices, sample_pool = None, replace=2) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Returns batch + targets\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Any, Any]: [description]\n",
    "        \"\"\"\n",
    "        default_coords = (self.target_dataset.grid_size // 2, self.target_dataset.grid_size // 2)\n",
    "        if sample_pool is not None:\n",
    "            batch = sample_pool[sampled_indices]\n",
    "        else:\n",
    "            batch = [None for _ in range(len(sampled_indices))]\n",
    "        for i in range(len(sampled_indices)):\n",
    "            if batch[i] is None:\n",
    "                batch[i] = (self.nca.generate_seed(1)[0].to(self.device), default_coords)\n",
    "            elif torch.sum(self.nca.alive(batch[i][0].unsqueeze(0))) == 0.0:\n",
    "                batch[i] = (self.nca.generate_seed(1)[0].to(self.device), default_coords)\n",
    "        for i in range(replace):\n",
    "            batch[-i] = (self.nca.generate_seed(1)[0].to(self.device), default_coords)\n",
    "        return batch\n",
    "\n",
    "    def sample_targets(self, batch_coords, directions, steps, substrates):\n",
    "        out_targets = []\n",
    "        goals = []\n",
    "        out_coords = []\n",
    "        \n",
    "        for coords, direction, num_steps, substrate in zip(batch_coords, directions, steps, substrates):\n",
    "            if direction == 0:\n",
    "                # stay in place\n",
    "                new_coords = coords\n",
    "            if direction == 1:\n",
    "                # left\n",
    "                new_coords = (coords[0] - (num_steps // 8), coords[1])\n",
    "            if direction == 2:\n",
    "                # right\n",
    "                new_coords = (coords[0] + (num_steps // 8), coords[1])\n",
    "            if direction == 3:\n",
    "                # up\n",
    "                new_coords = (coords[0], coords[1] - (num_steps // 8))\n",
    "            if direction == 4:\n",
    "                # down\n",
    "                new_coords = (coords[0], coords[1] + (num_steps // 8))\n",
    "            sub = substrate\n",
    "\n",
    "            if sub is not None:\n",
    "#                 zeros = torch.zeros(sub.size(), device=sub.device)\n",
    "#                 shift = num_steps // 16\n",
    "#                 if direction == 0:\n",
    "#                     # stay in place\n",
    "#                     zeros = sub.detach()\n",
    "#                 if direction == 1:\n",
    "#                     # left\n",
    "                    \n",
    "#                     zeros[\n",
    "#                     new_coords = (coords[0] - (num_steps // 16), coords[1])\n",
    "#                 if direction == 2:\n",
    "#                     # right\n",
    "#                     new_coords = (coords[0] + (num_steps // 16), coords[1])\n",
    "#                 if direction == 3:\n",
    "#                     # up\n",
    "#                     new_coords = (coords[0], coords[1] - (num_steps // 16))\n",
    "#                 if direction == 4:\n",
    "#                     # down\n",
    "#                     new_coords = (coords[0], coords[1] + (num_steps // 16))\n",
    "                x,y = coords\n",
    "                min_x = x - 16\n",
    "                min_x_diff = 0 - min(0, min_x)\n",
    "                max_x = x + 16\n",
    "                max_x_diff = 64 - max(64, max_x)\n",
    "                min_x = min_x + max_x_diff + min_x_diff\n",
    "                max_x = max_x + max_x_diff + min_x_diff\n",
    "\n",
    "                min_y = y - 16\n",
    "                min_y_diff = 0 - min(0, min_y)\n",
    "                max_y = y + 16\n",
    "                max_y_diff = 64 - max(64, max_y)\n",
    "                min_y = min_y + max_y_diff + min_y_diff\n",
    "                max_y = max_y + max_y_diff + min_y_diff\n",
    "\n",
    "                sub = substrate[:, min_x:max_x, min_y:max_y].clone()\n",
    "                sub = sub.unsqueeze(0)\n",
    "\n",
    "            target, new_coords = self.target_dataset.draw(new_coords[0], new_coords[1], sub)\n",
    "\n",
    "            goals.append(torch.tensor(direction, device=self.device))            \n",
    "            out_coords.append(new_coords)\n",
    "            out_targets.append(target)\n",
    "        \n",
    "        return out_coords, out_targets, goals\n",
    "\n",
    "    def train_batch(self, batch, directions, num_steps, use_substrate_as_targets = False):\n",
    "        coords = []\n",
    "        substrates = []\n",
    "        steps = [num_steps]*len(batch)\n",
    "        for i in range(len(batch)):\n",
    "            substrate, batch_coords = batch[i]\n",
    "            substrates.append(substrate.to(self.device))\n",
    "            coords.append(batch_coords)\n",
    "        if use_substrate_as_targets:\n",
    "            new_coords, targets, goals = self.sample_targets(coords, directions, steps, substrates=substrates)\n",
    "        else:\n",
    "            new_coords, targets, goals = self.sample_targets(coords, directions, steps, substrates=[None]*len(batch))\n",
    "\n",
    "        substrates = torch.stack(substrates, dim=0).squeeze()\n",
    "        targets = torch.stack(targets, dim=0).squeeze().to(self.device)\n",
    "        goals  = self.nca.encoder(torch.stack(goals, dim=0).squeeze().to(self.device))\n",
    "\n",
    "        substrates = self.nca.grow(substrates, num_steps=num_steps, goal=goals)\n",
    "        if not use_substrate_as_targets:\n",
    "            loss = self.loss(substrates, targets).mean()\n",
    "        else:\n",
    "            loss = self.loss(substrates, targets).mean()\n",
    "            # loss = F.mse_loss(\n",
    "            #     substrates, targets, reduction=\"none\"\n",
    "            # ).mean(dim=(1, 2, 3)).mean()\n",
    "        loss = self.loss(substrates, targets).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for p in self.nca.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad /= torch.norm(p.grad) + 1e-10\n",
    "        self.optimizer.step()\n",
    "        grad_dict = {}\n",
    "        for n, W in self.nca.named_parameters():\n",
    "            if W.grad is not None:\n",
    "                grad_dict[\"{}_grad\".format(n)] = float(torch.sum(W.grad).item())\n",
    "\n",
    "        return (\n",
    "            substrates.detach(),\n",
    "            list(zip(list(substrates.detach().cpu()), new_coords)),\n",
    "            targets,\n",
    "            loss.item(),\n",
    "            {\"loss\": loss.item(), \"log10loss\": math.log10(loss.item()), **grad_dict},\n",
    "        )\n",
    "\n",
    "#     def train_batch(self, batch, targets, goals):\n",
    "#         batch = self.nca.grow(batch, num_steps=num_steps, goal=goals)\n",
    "#         loss = self.loss(batch, target_images).mean()\n",
    "#         self.optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         for p in self.nca.parameters():\n",
    "#             if p.grad is not None:\n",
    "#                 p.grad /= torch.norm(p.grad) + 1e-10\n",
    "#         self.optimizer.step()\n",
    "#         self.lr_sched.step()\n",
    "#         grad_dict = {}\n",
    "#         for n, W in self.nca.named_parameters():\n",
    "#             if W.grad is not None:\n",
    "#                 grad_dict[\"{}_grad\".format(n)] = float(torch.sum(W.grad).item())\n",
    "\n",
    "#         return (\n",
    "#             batch.detach(),\n",
    "#             loss.item(),\n",
    "#             {\"loss\": loss.item(), \"log10loss\": math.log10(loss.item()), **grad_dict},\n",
    "#         )\n",
    "\n",
    "    def train(self, batch_size, epochs):\n",
    "        bar =  tqdm.tqdm(range(epochs))\n",
    "        self.pool = SamplePool(self.pool_size)\n",
    "\n",
    "        for i in bar:\n",
    "            idxs = random.sample(range(self.pool_size), batch_size)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                batch = self.sample_batch(idxs, self.pool, replace=4)\n",
    "\n",
    "            # train center\n",
    "            directions = [0]*batch_size\n",
    "            num_steps = np.random.randint(self.min_steps, self.max_steps)\n",
    "            substrates, new_batch, center_targets,  loss, metrics = self.train_batch(batch, directions, num_steps)\n",
    "            \n",
    "#             # train small random directions\n",
    "            directions = np.random.randint(1, 5, batch_size)\n",
    "            substrates, small_steps_batch, small_targets, loss, metrics = self.train_batch(new_batch, directions, 32, use_substrate_as_targets=False)\n",
    "            substrates, med_steps_batch, med_targets, loss, metrics = self.train_batch(small_steps_batch, directions, 32, use_substrate_as_targets=False)\n",
    "            # substrates, med_steps_batch, med_targets, loss, metrics = self.train_batch(small_steps_batch, directions, 1, use_substrate_as_targets=False)\n",
    "            substrates, large_steps_batch, large_targets, loss, metrics = self.train_batch(med_steps_batch, directions, 32, use_substrate_as_targets=False)\n",
    "#             # train medium random directions\n",
    "#             directions = np.random.randint(1, 5, batch_size)\n",
    "            self.pool[idxs] = small_steps_batch\n",
    "#             # train large random directions\n",
    "#             directions = np.random.randint(1, 5, batch_size)\n",
    "#             substrates, large_steps_batch, large_targets, loss, metrics = self.train_batch(med_steps_batch, directions, 8)\n",
    "\n",
    "#             # train largest random directions\n",
    "#             directions = np.random.randint(1, 5, batch_size)\n",
    "#             substrates, largest_steps_batch, largest_targets, loss, metrics = self.train_batch(large_steps_batch, directions, 8)\n",
    "\n",
    "            self.lr_sched.step()\n",
    "\n",
    "            description = \"--\".join([\"{}:{}\".format(k, metrics[k]) for k in metrics])\n",
    "            bar.set_description(description)\n",
    "            outputs = [\n",
    "                (\"sampled_batch\", batch, None),\n",
    "                (\"centered\", new_batch, center_targets),\n",
    "                (\"small\", small_steps_batch, small_targets),\n",
    "                (\"med\", med_steps_batch, med_targets),\n",
    "                (\"large\", large_steps_batch, large_targets)\n",
    "            ]\n",
    "            self.emit_metrics(i, outputs, metrics)\n",
    "\n",
    "\n",
    "    def emit_metrics(self, i: int, outputs, metrics={}):\n",
    "        with torch.no_grad():\n",
    "            for o in outputs:\n",
    "                keys, batch, targets = o\n",
    "                batch = [b[0].detach().cpu() for b in batch]\n",
    "                batch = torch.stack(batch, dim=0)\n",
    "                self.train_writer.add_images(\n",
    "                    keys + \"_batch\", self.to_rgb(batch), i, dataformats=\"NCHW\"\n",
    "                )\n",
    "                if targets is not None:\n",
    "                    self.train_writer.add_images(\n",
    "                        keys + \"_targets\", self.to_rgb(targets), i, dataformats=\"NCHW\"\n",
    "                    )\n",
    "\n",
    "\n",
    "            for k in metrics:\n",
    "                self.train_writer.add_scalar(k, metrics[k], i)\n",
    "            \n",
    "\n",
    "#     def train(self, batch_size, epochs, *args, **kwargs):\n",
    "#         self.pool = SamplePool(self.pool_size)\n",
    "#         bar = tqdm.tqdm(range(epochs))\n",
    "#         for i in bar:\n",
    "#             # Sort by loss, descending.\n",
    "#             idxs = random.sample(range(len(self.pool)), batch_size)\n",
    "#             with torch.no_grad():\n",
    "#                 targets, goal_vectors = self.sample_targets([0]*batch_size)\n",
    "#                 batch = self.sample_batch(idxs, self.pool)\n",
    "#                 batch[: (batch_size // 3)] = self.nca.generate_seed(batch_size // 3).to(\n",
    "#                     self.device\n",
    "#                 )\n",
    "\n",
    "#             # grow centered\n",
    "#             outputs, loss, metrics = self.train_batch(batch, targets)\n",
    "#             # train more\n",
    "#             outputs, loss, metrics = self.train_batch(batch, targets)\n",
    "            \n",
    "#             # switch goals\n",
    "#             with torch.no_grad():\n",
    "#                 targets, random_indices = self.sample_targets(idxs)\n",
    "#             outputs, loss, metrics = self.train_batch(outputs, targets)\n",
    "#             # train more\n",
    "#             outputs, loss, metrics = self.train_batch(outputs, targets)\n",
    "\n",
    "#             # Place outputs back in the pool.\n",
    "#             self.update_pool(idxs, outputs, targets)\n",
    "#             description = \"--\".join([\"{}:{}\".format(k, metrics[k]) for k in metrics])\n",
    "#             bar.set_description(description)\n",
    "#             self.emit_metrics(i, batch, outputs, targets[0], loss, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c137eb36-b133-44c7-a34b-6b9bc94afe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Embedding\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, out_channels: int, embedding_dim: int = 32):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = Embedding(num_embeddings, out_channels)\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Linear(embedding_dim, 64),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(64, out_channels)\n",
    "#         )\n",
    "    \n",
    "    def forward(self, indices):\n",
    "        embeddings = self.embedding(indices)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4126243-291f-4839-9518-05fe536f20b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HIDDEN_CHANNELS = 32\n",
    "\n",
    "encoder = Encoder(5, NUM_HIDDEN_CHANNELS, 32)\n",
    "nca =  ControllableImageNCA(target_shape=dataset.target_size(), encoder = encoder, living_channel_dim=3, num_hidden_channels=NUM_HIDDEN_CHANNELS, cell_fire_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7abaf57-39c4-44a3-add0-33e3cd1652d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "nca = nca.to(device)\n",
    "dataset.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58c8978a-d0dc-4700-84a0-ce5a9843d62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to tensorboard_logs/2022-02-18 07:12:16.832004\n"
     ]
    }
   ],
   "source": [
    "trainer = ControllableNCAImageTrainer(nca, dataset, nca_steps=[48, 64], lr=1e-3, num_damaged=0, damage_radius=3, device=device, pool_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2037b257-dd87-4bcd-aaca-bfa5aea1299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train(batch_size=24, epochs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07148d74-a534-4a58-a623-94fdb530b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Event, Thread\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from ipycanvas import Canvas, hold_canvas  # noqa\n",
    "from ipywidgets import Button, HBox, VBox\n",
    "\n",
    "from controllable_nca.utils import create_2d_circular_mask, rgb\n",
    "\n",
    "\n",
    "def to_numpy_rgb(x, use_rgb=False):\n",
    "    return rearrange(\n",
    "        np.squeeze(rgb(x, use_rgb).detach().cpu().numpy()), \"c x y -> x y c\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ControllableNCAImageVisualizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        trainer,\n",
    "        image_size,\n",
    "        rgb: bool = False,\n",
    "        canvas_scale=5,\n",
    "        damage_radius: int = 5,\n",
    "    ):\n",
    "        self.trainer = trainer\n",
    "        self.current_state = None\n",
    "        self.current_embedding = None\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.rgb = rgb\n",
    "        self.canvas_scale = canvas_scale\n",
    "        self.canvas_size = self.image_size * self.canvas_scale\n",
    "\n",
    "        self.canvas = Canvas(width=self.canvas_size, height=self.canvas_size)\n",
    "        self.canvas.on_mouse_down(self.handle_mouse_down)\n",
    "        self.stopped = Event()\n",
    "\n",
    "        x = self.trainer.target_dataset.x\n",
    "        # with torch.no_grad():\n",
    "        #     self.embeddings = self.trainer.target_dataset.one_hot\n",
    "        self.current_embedding = self.trainer.nca.encoder(torch.tensor(0, device=self.trainer.device))\n",
    "\n",
    "        self.device = self.trainer.device\n",
    "        self.damage_radius = damage_radius\n",
    "        self.current_state = self.trainer.nca.generate_seed(1).to(self.device)\n",
    "        \n",
    "        def button_fn(class_num):\n",
    "            def start(btn):\n",
    "                self.current_embedding = self.trainer.nca.encoder(torch.tensor(class_num, device=self.trainer.device))\n",
    "                if self.stopped.isSet():\n",
    "                    self.stopped.clear()\n",
    "                    Thread(target=self.loop).start()\n",
    "\n",
    "            return start\n",
    "\n",
    "        button_list = []\n",
    "        for i in range(5):\n",
    "            button_list.append(Button(description=\"{}\".format(i)))\n",
    "            button_list[-1].on_click(button_fn(i))\n",
    "\n",
    "        self.vbox = VBox(button_list)\n",
    "\n",
    "        self.stop_btn = Button(description=\"Stop\")\n",
    "\n",
    "        def stop(btn):\n",
    "            if not self.stopped.isSet():\n",
    "                self.stopped.set()\n",
    "\n",
    "        self.stop_btn.on_click(stop)\n",
    "\n",
    "    def handle_mouse_down(self, xpos, ypos):\n",
    "        in_x = int(xpos / self.canvas_scale)\n",
    "        in_y = int(ypos / self.canvas_scale)\n",
    "\n",
    "        mask = create_2d_circular_mask(\n",
    "            self.image_size,\n",
    "            self.image_size,\n",
    "            (in_x, in_y),\n",
    "            radius=self.damage_radius,\n",
    "        )\n",
    "        self.current_state[0][:, mask] *= 0.0\n",
    "\n",
    "    def draw_image(self, rgb):\n",
    "        with hold_canvas(self.canvas):\n",
    "            rgb = np.squeeze(rearrange(rgb, \"b c w h -> b w h c\"))\n",
    "            self.canvas.clear()  # Clear the old animation step\n",
    "            self.canvas.put_image_data(\n",
    "                cv2.resize(\n",
    "                    rgb * 255.0,\n",
    "                    (self.canvas_size, self.canvas_size),\n",
    "                    interpolation=cv2.INTER_NEAREST,\n",
    "                ),\n",
    "                0,\n",
    "                0,\n",
    "            )\n",
    "\n",
    "    def loop(self):\n",
    "        with torch.no_grad():\n",
    "            self.current_state = self.trainer.nca.generate_seed(1).to(self.device)\n",
    "            while not self.stopped.wait(0.02):  # the first call is in `interval` secs\n",
    "                # update_particle_locations()\n",
    "                self.draw_image(self.trainer.to_rgb(self.current_state))\n",
    "                self.current_state = self.trainer.nca.grow(\n",
    "                    self.current_state, 1, self.current_embedding\n",
    "                )\n",
    "\n",
    "    def visualize(self):\n",
    "        Thread(target=self.loop).start()\n",
    "        display(self.canvas, HBox([self.stop_btn, self.vbox]))  # noqa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2866927-4f84-49ab-97ba-9fbc4d39faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nca.load(\"newest.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "508f515f-63dc-4ea3-aacd-e9725b3a1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = ControllableNCAImageVisualizer(trainer, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22f656b2-6fc1-4ea9-9be8-397f650c6043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ControllableImageNCA(\n",
       "  (perception_net): Conv2d(37, 111, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=37, bias=False)\n",
       "  (update_net): UpdateNet(\n",
       "    (out): Sequential(\n",
       "      (0): Conv2d(111, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 37, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(5, 32)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88898ef3-ed72-444c-9d53-88d4e6b0f469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a8051be07743a0b90f4be6f8af6d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=320, width=320)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc9b950a7fd4bf197135bc4008dc691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Stop', style=ButtonStyle()), VBox(children=(Button(description='0', style=Bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pycanvas] *",
   "language": "python",
   "name": "conda-env-pycanvas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
